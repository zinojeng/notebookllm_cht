import asyncio
import edge_tts
from pydub import AudioSegment
from deep_translator import GoogleTranslator
import os
import re
import json
from typing import List, Dict, Tuple
import soundfile as sf
import numpy as np
from datetime import datetime
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# æ–°å¢ API æ”¯æ´
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

class AudioProcessor:
    def __init__(self):
        """åˆå§‹åŒ–éŸ³é »è™•ç†å™¨"""
        # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
        self.openai_api_key = os.getenv('OPENAI_API_KEY')
        self.gemini_api_key = os.getenv('GEMINI_API_KEY') or os.getenv('GOOGLE_API_KEY')
        
        # è¨­å®šç¿»è­¯æä¾›å•†
        self.translation_provider = os.getenv('TRANSLATION_PROVIDER', 'google').lower()
        
        # åˆå§‹åŒ–ç¿»è­¯å™¨
        self._init_translators()
        
        # ä¸­æ–‡èªéŸ³è¨­å®š - ä½¿ç”¨æ›´è‡ªç„¶çš„è²éŸ³
        self.chinese_voices = {
            'female': os.getenv('EDGE_TTS_VOICE_FEMALE', 'zh-TW-HsiaoChenNeural'),
            'male': os.getenv('EDGE_TTS_VOICE_MALE', 'zh-TW-YunJheNeural')
        }
        
        print(f"ğŸ”§ éŸ³é »è™•ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç¿»è­¯æä¾›å•†: {self.translation_provider}")
        print(f"   å¥³æ€§è²éŸ³: {self.chinese_voices['female']}")
        print(f"   ç”·æ€§è²éŸ³: {self.chinese_voices['male']}")
    
    def _init_translators(self):
        """åˆå§‹åŒ–ç¿»è­¯å™¨"""
        self.translator = GoogleTranslator(source='en', target='zh-tw')
        
        # å¾ç’°å¢ƒè®Šæ•¸ç²å–æ¨¡å‹åç¨±
        self.openai_model = os.getenv('OPENAI_MODEL', 'o1-mini')
        self.gemini_model_name = os.getenv('GEMINI_MODEL', 'gemini-2.0-flash-exp')
        
        # åˆå§‹åŒ– OpenAI
        if self.openai_api_key and OPENAI_AVAILABLE:
            self.openai_client = openai.OpenAI(api_key=self.openai_api_key)
            print(f"âœ… OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ (æ¨¡å‹: {self.openai_model})")
        else:
            self.openai_client = None
            if self.translation_provider == 'openai':
                print("âš ï¸  OpenAI API é‡‘é‘°æœªè¨­å®šï¼Œå°‡ä½¿ç”¨ Google ç¿»è­¯")
                self.translation_provider = 'google'
        
        # åˆå§‹åŒ– Gemini
        if self.gemini_api_key and GEMINI_AVAILABLE:
            genai.configure(api_key=self.gemini_api_key)
            self.gemini_model = genai.GenerativeModel(self.gemini_model_name)
            print(f"âœ… Gemini å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ (æ¨¡å‹: {self.gemini_model_name})")
        else:
            self.gemini_model = None
            if self.translation_provider == 'gemini':
                print("âš ï¸  Gemini API é‡‘é‘°æœªè¨­å®šï¼Œå°‡ä½¿ç”¨ Google ç¿»è­¯")
                self.translation_provider = 'google'
    
    def translate_with_ai(self, text: str, context: Dict = None) -> str:
        """ä½¿ç”¨ AI æ¨¡å‹é€²è¡Œæ™ºèƒ½ç¿»è­¯"""
        if self.translation_provider == 'openai' and self.openai_client:
            return self._translate_with_openai(text, context)
        elif self.translation_provider == 'gemini' and self.gemini_model:
            return self._translate_with_gemini(text, context)
        else:
            # å›é€€åˆ° Google ç¿»è­¯
            return self.translator.translate(text)
    
    def _translate_with_openai(self, text: str, context: Dict = None) -> str:
        """ä½¿ç”¨ OpenAI o1-mini é€²è¡Œç¿»è­¯"""
        try:
            # æ§‹å»ºæç¤ºè©
            system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è‹±æ–‡åˆ°ç¹é«”ä¸­æ–‡ç¿»è­¯å°ˆå®¶ï¼Œå°ˆé–€è™•ç† Podcast å°è©±å…§å®¹ã€‚

ç¿»è­¯è¦æ±‚ï¼š
1. ä¿æŒå°è©±çš„è‡ªç„¶æ€§å’Œå£èªåŒ–ç‰¹è‰²
2. æ ¹æ“šèªªè©±è€…è§’è‰²èª¿æ•´èªæ°£ï¼ˆä¸»æŒäºº vs å˜‰è³“ï¼‰
3. ä¿ç•™å°è©±ä¸­çš„èªæ°£è©å’Œè½‰æŠ˜è©
4. ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡çš„è¡¨é”ç¿’æ…£
5. å°æ–¼å°ˆæ¥­è¡“èªï¼Œæä¾›è‡ªç„¶çš„ä¸­æ–‡è¡¨é”

è«‹ç›´æ¥è¿”å›ç¿»è­¯çµæœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚"""

            user_prompt = f"è«‹å°‡ä»¥ä¸‹è‹±æ–‡å°è©±ç¿»è­¯æˆè‡ªç„¶çš„ç¹é«”ä¸­æ–‡ï¼š\n\n{text}"
            
            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
            if context:
                if context.get('is_question'):
                    user_prompt += "\n\næ³¨æ„ï¼šé€™æ˜¯ä¸€å€‹å•å¥ï¼Œè«‹ç¢ºä¿ç¿»è­¯å¾Œä¿æŒç–‘å•èªæ°£ã€‚"
                elif context.get('is_response'):
                    user_prompt += "\n\næ³¨æ„ï¼šé€™æ˜¯å°å‰é¢å•é¡Œçš„å›æ‡‰ï¼Œè«‹ä½¿ç”¨è‡ªç„¶çš„å›ç­”èªæ°£ã€‚"
                elif context.get('is_transition'):
                    user_prompt += "\n\næ³¨æ„ï¼šé€™æ˜¯è©±é¡Œè½‰æ›ï¼Œè«‹ä½¿ç”¨é©ç•¶çš„è½‰å ´è¡¨é”ã€‚"
            
            response = self.openai_client.chat.completions.create(
                model=self.openai_model,
                messages=[
                    {"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}
                ],
                max_completion_tokens=1000,
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"âš ï¸  OpenAI ç¿»è­¯å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨ç¿»è­¯: {e}")
            return self.translator.translate(text)
    
    def _translate_with_gemini(self, text: str, context: Dict = None) -> str:
        """ä½¿ç”¨ Gemini 2.0 Flash Preview é€²è¡Œç¿»è­¯"""
        try:
            # æ§‹å»ºæç¤ºè©
            prompt = f"""ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„è‹±æ–‡åˆ°ç¹é«”ä¸­æ–‡ç¿»è­¯å°ˆå®¶ï¼Œå°ˆé–€è™•ç† Podcast å°è©±å…§å®¹ã€‚

ç¿»è­¯è¦æ±‚ï¼š
1. ä¿æŒå°è©±çš„è‡ªç„¶æ€§å’Œå£èªåŒ–ç‰¹è‰²
2. æ ¹æ“šèªªè©±è€…è§’è‰²èª¿æ•´èªæ°£
3. ä¿ç•™å°è©±ä¸­çš„èªæ°£è©å’Œè½‰æŠ˜è©
4. ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡çš„è¡¨é”ç¿’æ…£
5. å°æ–¼å°ˆæ¥­è¡“èªï¼Œæä¾›è‡ªç„¶çš„ä¸­æ–‡è¡¨é”

è«‹å°‡ä»¥ä¸‹è‹±æ–‡å°è©±ç¿»è­¯æˆè‡ªç„¶çš„ç¹é«”ä¸­æ–‡ï¼š

{text}

è«‹ç›´æ¥è¿”å›ç¿»è­¯çµæœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚"""

            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
            if context:
                if context.get('is_question'):
                    prompt += "\n\næ³¨æ„ï¼šé€™æ˜¯ä¸€å€‹å•å¥ï¼Œè«‹ç¢ºä¿ç¿»è­¯å¾Œä¿æŒç–‘å•èªæ°£ã€‚"
                elif context.get('is_response'):
                    prompt += "\n\næ³¨æ„ï¼šé€™æ˜¯å°å‰é¢å•é¡Œçš„å›æ‡‰ï¼Œè«‹ä½¿ç”¨è‡ªç„¶çš„å›ç­”èªæ°£ã€‚"
                elif context.get('is_transition'):
                    prompt += "\n\næ³¨æ„ï¼šé€™æ˜¯è©±é¡Œè½‰æ›ï¼Œè«‹ä½¿ç”¨é©ç•¶çš„è½‰å ´è¡¨é”ã€‚"
            
            response = self.gemini_model.generate_content(prompt)
            return response.text.strip()
            
        except Exception as e:
            print(f"âš ï¸  Gemini ç¿»è­¯å¤±æ•—ï¼Œä½¿ç”¨å‚™ç”¨ç¿»è­¯: {e}")
            return self.translator.translate(text)
    
    def load_audio(self, file_path: str) -> AudioSegment:
        """è¼‰å…¥éŸ³é »æ–‡ä»¶"""
        try:
            audio = AudioSegment.from_wav(file_path)
            print(f"âœ… æˆåŠŸè¼‰å…¥éŸ³é »: {file_path}")
            print(f"   æ™‚é•·: {len(audio)/1000:.2f} ç§’")
            print(f"   æ¡æ¨£ç‡: {audio.frame_rate} Hz")
            return audio
        except Exception as e:
            print(f"âŒ è¼‰å…¥éŸ³é »å¤±æ•—: {e}")
            return None
    
    def transcribe_with_timestamps(self, audio_path: str) -> Dict:
        """ä½¿ç”¨ OpenAI Whisper API é€²è¡ŒèªéŸ³è­˜åˆ¥ï¼Œä¿ç•™æ™‚é–“æˆ³"""
        try:
            print("ğŸ¯ é–‹å§‹èªéŸ³è­˜åˆ¥...")
            
            if not self.openai_client:
                print("âŒ OpenAI å®¢æˆ¶ç«¯æœªåˆå§‹åŒ–ï¼Œç„¡æ³•é€²è¡ŒèªéŸ³è­˜åˆ¥")
                return None
            
            # ä½¿ç”¨ OpenAI Whisper API
            with open(audio_path, "rb") as audio_file:
                transcript = self.openai_client.audio.transcriptions.create(
                    model="whisper-1",
                    file=audio_file,
                    response_format="verbose_json",
                    timestamp_granularities=["segment"]
                )
            
            # è½‰æ›æ ¼å¼ä»¥ç¬¦åˆåŸæœ‰çš„çµæ§‹
            segments = []
            for segment in transcript.segments:
                segments.append({
                    'start': segment['start'],
                    'end': segment['end'],
                    'text': segment['text'].strip(),
                    'words': []  # OpenAI API ä¸æä¾›è©ç´šæ™‚é–“æˆ³
                })
            
            print(f"âœ… èªéŸ³è­˜åˆ¥å®Œæˆï¼Œå…± {len(segments)} å€‹ç‰‡æ®µ")
            return {
                'text': transcript.text,
                'segments': segments,
                'language': transcript.language or 'en'
            }
            
        except Exception as e:
            print(f"âŒ èªéŸ³è­˜åˆ¥å¤±æ•—: {e}")
            # å¦‚æœ OpenAI API å¤±æ•—ï¼Œå˜—è©¦ä½¿ç”¨ç°¡å–®çš„åˆ†æ®µæ–¹æ³•
            return self._fallback_transcription(audio_path)
    
    def detect_speakers_and_dialogue(self, segments: List[Dict]) -> List[Dict]:
        """æª¢æ¸¬å°è©±æ¨¡å¼ä¸¦æ¨™è¨˜èªªè©±è€…"""
        dialogue_segments = []
        
        for i, segment in enumerate(segments):
            text = segment['text'].strip()
            
            # æª¢æ¸¬å°è©±ç‰¹å¾µ
            is_question = text.endswith('?') or any(word in text.lower() for word in ['what', 'how', 'why', 'when', 'where', 'who'])
            is_response = any(word in text.lower() for word in ['well', 'actually', 'so', 'yeah', 'right', 'exactly'])
            is_transition = any(phrase in text.lower() for phrase in ['speaking of', 'by the way', 'another thing', 'also'])
            
            # ç°¡å–®çš„èªªè©±è€…æª¢æ¸¬ï¼ˆåŸºæ–¼å°è©±æ¨¡å¼ï¼‰
            if i == 0:
                speaker = 'A'
            elif is_question and dialogue_segments and dialogue_segments[-1]['speaker'] == 'A':
                speaker = 'B'
            elif is_response and dialogue_segments and dialogue_segments[-1]['speaker'] == 'B':
                speaker = 'A'
            else:
                # ä¿æŒå‰ä¸€å€‹èªªè©±è€…ï¼Œé™¤éæœ‰æ˜é¡¯çš„è½‰æ›
                speaker = dialogue_segments[-1]['speaker'] if dialogue_segments else 'A'
                if len(text) > 100 and i > 0:  # é•·å¥å­å¯èƒ½æ˜¯èªªè©±è€…è½‰æ›
                    speaker = 'B' if speaker == 'A' else 'A'
            
            dialogue_segments.append({
                **segment,
                'speaker': speaker,
                'is_question': is_question,
                'is_response': is_response,
                'is_transition': is_transition
            })
        
        print(f"âœ… å°è©±åˆ†æå®Œæˆï¼Œæª¢æ¸¬åˆ° {len(set(seg['speaker'] for seg in dialogue_segments))} å€‹èªªè©±è€…")
        return dialogue_segments
    
    def translate_with_context(self, segments: List[Dict]) -> List[Dict]:
        """ç¿»è­¯æ–‡æœ¬ï¼Œä¿æŒå°è©±çš„è‡ªç„¶æ€§"""
        translated_segments = []
        
        print("ğŸŒ é–‹å§‹ç¿»è­¯...")
        print(f"   ä½¿ç”¨ç¿»è­¯æä¾›å•†: {self.translation_provider}")
        
        for i, segment in enumerate(segments):
            try:
                original_text = segment['text']
                
                # ä¿ç•™å°è©±çš„è‡ªç„¶è¡¨é”
                # é è™•ç†ï¼šä¿ç•™èªæ°£è©å’Œå°è©±æ¨™è¨˜
                text_to_translate = original_text
                
                # æº–å‚™ä¸Šä¸‹æ–‡ä¿¡æ¯
                context = {
                    'is_question': segment.get('is_question', False),
                    'is_response': segment.get('is_response', False),
                    'is_transition': segment.get('is_transition', False),
                    'speaker': segment.get('speaker', 'A')
                }
                
                # ä½¿ç”¨ AI ç¿»è­¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰æˆ–å›é€€åˆ° Google ç¿»è­¯
                translated = self.translate_with_ai(text_to_translate, context)
                
                # å¾Œè™•ç†ï¼šèª¿æ•´ä¸­æ–‡è¡¨é”ä½¿å…¶æ›´è‡ªç„¶
                translated = self.enhance_chinese_dialogue(translated, segment)
                
                translated_segments.append({
                    **segment,
                    'original_text': original_text,
                    'translated_text': translated
                })
                
                print(f"   {i+1}/{len(segments)} - ç¿»è­¯å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ ç¿»è­¯ç¬¬ {i+1} æ®µå¤±æ•—: {e}")
                translated_segments.append({
                    **segment,
                    'original_text': segment['text'],
                    'translated_text': segment['text']  # ä¿ç•™åŸæ–‡
                })
        
        print("âœ… ç¿»è­¯å®Œæˆ")
        return translated_segments
    
    def enhance_chinese_dialogue(self, translated_text: str, segment: Dict) -> str:
        """å¢å¼·ä¸­æ–‡å°è©±çš„è‡ªç„¶æ€§"""
        text = translated_text
        
        # æ ¹æ“šå°è©±é¡å‹èª¿æ•´è¡¨é”
        if segment.get('is_question'):
            # å•å¥èª¿æ•´
            if not text.endswith('ï¼Ÿ') and not text.endswith('?'):
                text += 'ï¼Ÿ'
            # æ·»åŠ è‡ªç„¶çš„å•å¥é–‹é ­
            question_starters = ['é‚£éº¼', 'æ‰€ä»¥', 'é‚£', 'å—¯']
            if not any(text.startswith(starter) for starter in question_starters):
                if 'ä»€éº¼' in text or 'æ€éº¼' in text:
                    text = 'é‚£' + text
        
        elif segment.get('is_response'):
            # å›æ‡‰èª¿æ•´
            response_starters = ['å—¯', 'å°', 'æ˜¯çš„', 'æ²’éŒ¯', 'ç¢ºå¯¦']
            if not any(text.startswith(starter) for starter in response_starters):
                text = 'å—¯ï¼Œ' + text
        
        elif segment.get('is_transition'):
            # è½‰å ´èª¿æ•´
            transition_words = ['èªªåˆ°é€™å€‹', 'é †ä¾¿èªªä¸€ä¸‹', 'å¦å¤–', 'é‚„æœ‰']
            if not any(phrase in text for phrase in transition_words):
                text = 'èªªåˆ°é€™å€‹ï¼Œ' + text
        
        # æ·»åŠ è‡ªç„¶çš„èªæ°£è©
        if len(text) > 50 and 'ï¼Œ' in text:
            # åœ¨é•·å¥ä¸­é–“æ·»åŠ èªæ°£è©
            parts = text.split('ï¼Œ')
            if len(parts) > 1:
                parts[0] += 'å‘¢'
                text = 'ï¼Œ'.join(parts)
        
        return text
    
    async def generate_chinese_audio(self, segments: List[Dict], output_dir: str) -> str:
        """ç”Ÿæˆä¸­æ–‡èªéŸ³"""
        os.makedirs(output_dir, exist_ok=True)
        audio_files = []
        
        print("ğŸ¤ é–‹å§‹ç”Ÿæˆä¸­æ–‡èªéŸ³...")
        
        for i, segment in enumerate(segments):
            try:
                # é¸æ“‡è²éŸ³ï¼ˆæ ¹æ“šèªªè©±è€…ï¼‰
                voice = self.chinese_voices['female'] if segment['speaker'] == 'A' else self.chinese_voices['male']
                
                # èª¿æ•´èªéŸ³åƒæ•¸ä»¥æ›´è‡ªç„¶
                text = segment['translated_text']
                
                # æ·»åŠ é©ç•¶çš„åœé “æ¨™è¨˜
                text = self.add_speech_marks(text, segment)
                
                # ç”ŸæˆèªéŸ³æ–‡ä»¶
                output_file = os.path.join(output_dir, f"segment_{i:03d}_{segment['speaker']}.wav")
                
                communicate = edge_tts.Communicate(text, voice)
                await communicate.save(output_file)
                
                audio_files.append({
                    'file': output_file,
                    'start': segment['start'],
                    'end': segment['end'],
                    'speaker': segment['speaker'],
                    'duration': segment['end'] - segment['start']
                })
                
                print(f"   {i+1}/{len(segments)} - èªéŸ³ç”Ÿæˆå®Œæˆ")
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆç¬¬ {i+1} æ®µèªéŸ³å¤±æ•—: {e}")
        
        # åˆä½µéŸ³é »æ–‡ä»¶
        final_audio_path = await self.merge_audio_segments(audio_files, output_dir)
        print("âœ… ä¸­æ–‡èªéŸ³ç”Ÿæˆå®Œæˆ")
        return final_audio_path
    
    def add_speech_marks(self, text: str, segment: Dict) -> str:
        """æ·»åŠ èªéŸ³æ¨™è¨˜ä»¥æ”¹å–„è‡ªç„¶åº¦"""
        # æ·»åŠ åœé “
        if segment.get('is_question'):
            text = text.replace('ï¼Ÿ', '<break time="500ms"/>ï¼Ÿ')
        
        # åœ¨é€—è™Ÿå¾Œæ·»åŠ çŸ­æš«åœé “
        text = text.replace('ï¼Œ', 'ï¼Œ<break time="300ms"/>')
        
        # åœ¨å¥è™Ÿå¾Œæ·»åŠ åœé “
        text = text.replace('ã€‚', 'ã€‚<break time="500ms"/>')
        
        # èª¿æ•´èªé€Ÿå’ŒéŸ³èª¿
        if segment.get('is_question'):
            text = f'<prosody rate="0.9" pitch="+5%">{text}</prosody>'
        elif segment.get('is_response'):
            text = f'<prosody rate="1.0" pitch="-2%">{text}</prosody>'
        
        return text
    
    async def merge_audio_segments(self, audio_files: List[Dict], output_dir: str) -> str:
        """åˆä½µéŸ³é »ç‰‡æ®µ"""
        try:
            combined = AudioSegment.empty()
            
            for audio_info in audio_files:
                # è¼‰å…¥éŸ³é »ç‰‡æ®µ
                segment_audio = AudioSegment.from_wav(audio_info['file'])
                
                # æ·»åŠ åˆ°åˆä½µéŸ³é »
                combined += segment_audio
                
                # æ·»åŠ é©ç•¶çš„é–“éš”ï¼ˆæ¨¡æ“¬è‡ªç„¶å°è©±ï¼‰
                if audio_info != audio_files[-1]:  # ä¸æ˜¯æœ€å¾Œä¸€å€‹ç‰‡æ®µ
                    silence_duration = min(1000, max(300, int(audio_info['duration'] * 100)))
                    combined += AudioSegment.silent(duration=silence_duration)
            
            # è¼¸å‡ºæœ€çµ‚æ–‡ä»¶
            final_path = os.path.join(output_dir, "chinese_podcast_final.wav")
            combined.export(final_path, format="wav")
            
            print(f"âœ… éŸ³é »åˆä½µå®Œæˆ: {final_path}")
            return final_path
            
        except Exception as e:
            print(f"âŒ éŸ³é »åˆä½µå¤±æ•—: {e}")
            return None
    
    def save_transcript(self, segments: List[Dict], output_path: str):
        """ä¿å­˜é€å­—ç¨¿"""
        try:
            transcript_data = {
                'timestamp': datetime.now().isoformat(),
                'total_segments': len(segments),
                'segments': []
            }
            
            for segment in segments:
                transcript_data['segments'].append({
                    'start_time': f"{segment['start']:.2f}s",
                    'end_time': f"{segment['end']:.2f}s",
                    'speaker': segment['speaker'],
                    'original_text': segment['original_text'],
                    'translated_text': segment['translated_text'],
                    'dialogue_type': {
                        'is_question': segment.get('is_question', False),
                        'is_response': segment.get('is_response', False),
                        'is_transition': segment.get('is_transition', False)
                    }
                })
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(transcript_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… é€å­—ç¨¿å·²ä¿å­˜: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜é€å­—ç¨¿å¤±æ•—: {e}")
    
    async def process_audio_complete(self, input_wav_path: str, output_dir: str = "output") -> Dict:
        """å®Œæ•´çš„éŸ³é »è™•ç†æµç¨‹"""
        print("ğŸš€ é–‹å§‹å®Œæ•´éŸ³é »è™•ç†æµç¨‹...")
        
        # 1. è¼‰å…¥éŸ³é »
        audio = self.load_audio(input_wav_path)
        if not audio:
            return None
        
        # 2. èªéŸ³è­˜åˆ¥
        transcription = self.transcribe_with_timestamps(input_wav_path)
        if not transcription:
            return None
        
        # 3. å°è©±åˆ†æ
        dialogue_segments = self.detect_speakers_and_dialogue(transcription['segments'])
        
        # 4. ç¿»è­¯
        translated_segments = self.translate_with_context(dialogue_segments)
        
        # 5. ç”Ÿæˆä¸­æ–‡èªéŸ³
        chinese_audio_path = await self.generate_chinese_audio(translated_segments, output_dir)
        
        # 6. ä¿å­˜é€å­—ç¨¿
        transcript_path = os.path.join(output_dir, "transcript.json")
        self.save_transcript(translated_segments, transcript_path)
        
        result = {
            'original_audio': input_wav_path,
            'chinese_audio': chinese_audio_path,
            'transcript': transcript_path,
            'segments_count': len(translated_segments),
            'total_duration': sum(seg['end'] - seg['start'] for seg in translated_segments)
        }
        
        print("ğŸ‰ éŸ³é »è™•ç†å®Œæˆï¼")
        print(f"   åŸå§‹éŸ³é »: {input_wav_path}")
        print(f"   ä¸­æ–‡éŸ³é »: {chinese_audio_path}")
        print(f"   é€å­—ç¨¿: {transcript_path}")
        print(f"   ç¸½æ™‚é•·: {result['total_duration']:.2f} ç§’")
        
        return result
    
    def _fallback_transcription(self, audio_path: str) -> Dict:
        """å‚™ç”¨è½‰éŒ„æ–¹æ³•ï¼Œç•¶ OpenAI API ä¸å¯ç”¨æ™‚ä½¿ç”¨"""
        try:
            print("âš ï¸  ä½¿ç”¨å‚™ç”¨è½‰éŒ„æ–¹æ³•...")
            
            # è¼‰å…¥éŸ³é »ä¸¦å‰µå»ºç°¡å–®çš„åˆ†æ®µ
            audio = AudioSegment.from_wav(audio_path)
            duration = len(audio) / 1000.0  # è½‰æ›ç‚ºç§’
            
            # å‰µå»ºå‡çš„åˆ†æ®µï¼ˆæ¯30ç§’ä¸€æ®µï¼‰
            segments = []
            segment_duration = 30.0
            num_segments = int(duration / segment_duration) + 1
            
            for i in range(num_segments):
                start_time = i * segment_duration
                end_time = min((i + 1) * segment_duration, duration)
                
                segments.append({
                    'start': start_time,
                    'end': end_time,
                    'text': f"[éŸ³é »ç‰‡æ®µ {i+1}] è«‹æ‰‹å‹•æ·»åŠ è½‰éŒ„å…§å®¹",
                    'words': []
                })
            
            print(f"âœ… å‚™ç”¨è½‰éŒ„å®Œæˆï¼Œå…± {len(segments)} å€‹ç‰‡æ®µ")
            return {
                'text': "è«‹æ‰‹å‹•æ·»åŠ å®Œæ•´è½‰éŒ„å…§å®¹",
                'segments': segments,
                'language': 'en'
            }
            
        except Exception as e:
            print(f"âŒ å‚™ç”¨è½‰éŒ„ä¹Ÿå¤±æ•—: {e}")
            return None 